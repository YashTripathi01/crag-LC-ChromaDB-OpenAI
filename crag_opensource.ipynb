{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g12a20srr1pP"
      },
      "source": [
        "# Open Source RAG Implementation\n",
        "\n",
        "## Using Hugging Face Transformers and ChromaDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wir_kAzd4q11"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi # Check GPU and CUDA version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZt8ovdzsovr"
      },
      "outputs": [],
      "source": [
        "# Install the dependencies\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install --quiet transformers sentence-transformers chromadb langchain-community pypdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLdqe3P3r-Zf"
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueizliv8rsh4"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import torch\n",
        "from transformers import GPTJForCausalLM, AutoTokenizer, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import hashlib\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check for GPU availability\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "logger.info(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bTnMs-sCT_"
      },
      "source": [
        "## 2. Initialize Models and Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ7JaR9RsD-f"
      },
      "outputs": [],
      "source": [
        "def initialize_models():\n",
        "    \"\"\"Initialize the LLM and embedding models\"\"\"\n",
        "\n",
        "    # Initialize GPT-J model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "    model = GPTJForCausalLM.from_pretrained(\n",
        "        \"openai-community/gpt2\",\n",
        "        torch_dtype=torch.float16,  # Use float16 for memory efficiency\n",
        "        # low_cpu_mem_usage=True\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Create a text generation pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=DEVICE,\n",
        "        max_length=2048,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "\n",
        "    # Initialize sentence transformer for embeddings\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    return generator, embedding_model\n",
        "\n",
        "\n",
        "def initialize_chromadb():\n",
        "    \"\"\"Initialize ChromaDB\"\"\"\n",
        "    client = chromadb.PersistentClient(path=\"/content/chromadb\")\n",
        "\n",
        "    # Create or get existing collection\n",
        "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "        model_name=\"all-MiniLM-L6-v2\"\n",
        "    )\n",
        "\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=\"docs\",\n",
        "        embedding_function=embedding_function,\n",
        "        metadata={\"description\": \"Documents for RAG\"},\n",
        "    )\n",
        "\n",
        "    return collection\n",
        "\n",
        "\n",
        "# Initialize\n",
        "logger.info(\"Initializing models and database...\")\n",
        "GENERATOR, EMBEDDING_MODEL = initialize_models()\n",
        "COLLECTION = initialize_chromadb()\n",
        "logger.info(\"Initialization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ8Mze2-sLs4"
      },
      "source": [
        "## 3. Enhanced Prompt Template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfVUT4XpsNrg"
      },
      "outputs": [],
      "source": [
        "ENHANCED_PROMPT_TEMPLATE = \"\"\"\n",
        "Context Information:\n",
        "{context}\n",
        "\n",
        "User Question: {question}\n",
        "\n",
        "Instructions: Using the context above, provide a clear and direct answer to the user's question. If the context doesn't contain enough information, acknowledge this. Use bullet points when appropriate.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vax7zifHsTdg"
      },
      "source": [
        "## 4. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqkjt8YdsVDc"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text: str, max_chunk_size: int = 1000) -> list[str]:\n",
        "    \"\"\"Chunk a single text into smaller pieces\"\"\"\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = paragraph\n",
        "        else:\n",
        "            current_chunk += \" \" + paragraph\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def read_and_process_documents(directory: str, collection):\n",
        "    \"\"\"Read PDFs and process them for the database\"\"\"\n",
        "    logger.info(f\"Reading documents from {directory}\")\n",
        "\n",
        "    # Load documents\n",
        "    loader = PyPDFDirectoryLoader(directory)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Process and index documents\n",
        "    batch_size = 100  # Process in batches\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i : i + batch_size]\n",
        "\n",
        "        # Chunk texts\n",
        "        all_chunks = []\n",
        "        for doc in batch:\n",
        "            chunks = chunk_text(doc.page_content)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        # Prepare data for ChromaDB\n",
        "        ids = [hashlib.sha256(chunk.encode()).hexdigest() for chunk in all_chunks]\n",
        "\n",
        "        # Add to collection\n",
        "        collection.add(documents=all_chunks, ids=ids)\n",
        "\n",
        "        logger.info(f\"Processed and indexed batch of {len(all_chunks)} chunks\")\n",
        "\n",
        "    logger.info(\"Document processing complete\")\n",
        "\n",
        "\n",
        "def prepare_context(query_results) -> str:\n",
        "    \"\"\"Prepare context from query results\"\"\"\n",
        "    context_parts = []\n",
        "\n",
        "    for i, (doc, distance) in enumerate(\n",
        "        zip(query_results[\"documents\"][0], query_results[\"distances\"][0]), 1\n",
        "    ):\n",
        "        relevance_score = 1 - distance  # Convert distance to similarity score\n",
        "        context_part = f\"[Excerpt {i} (Relevance: {relevance_score:.2f})]\\n{doc}\\n\"\n",
        "        context_parts.append(context_part)\n",
        "\n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "\n",
        "def answer_question(question: str, collection) -> str:\n",
        "    \"\"\"Process a question and return an answer\"\"\"\n",
        "    # Query ChromaDB\n",
        "    query_results = collection.query(\n",
        "        query_texts=[question], n_results=3  # Number of relevant chunks to retrieve\n",
        "    )\n",
        "\n",
        "    # Prepare context\n",
        "    context = prepare_context(query_results)\n",
        "\n",
        "    # Prepare prompt\n",
        "    prompt = ENHANCED_PROMPT_TEMPLATE.format(context=context, question=question)\n",
        "\n",
        "    # Generate response\n",
        "    response = GENERATOR(prompt, max_length=2048, num_return_sequences=1)\n",
        "\n",
        "    return response[0][\"generated_text\"].split(\"Answer:\")[1].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vSpMEXKsZER"
      },
      "source": [
        "## 5. Interactive Chat Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaLLxfGssaWw"
      },
      "outputs": [],
      "source": [
        "def run_interactive_chat(collection):\n",
        "    \"\"\"Run an interactive chat session\"\"\"\n",
        "    print(\"Welcome to the Open Source Chatbot! Type 'quit' to exit.\")\n",
        "    print(\"Note: This is using GPT-J, responses might take longer than with OpenAI.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \").strip()\n",
        "\n",
        "        if question.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Thank you for using the Chatbot. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not question:\n",
        "            print(\"Please ask a question!\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(\"Thinking...\")  # Add this because local models might be slower\n",
        "            answer = answer_question(question, collection)\n",
        "            print(\"\\nAnswer:\", answer)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing question: {e}\")\n",
        "            print(\n",
        "                \"I apologize, but I encountered an error processing your question. Please try again.\"\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67L4lBcBseNZ"
      },
      "source": [
        "## 6. Main Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7X_Mo01sefP"
      },
      "outputs": [],
      "source": [
        "docs_directory = \"/content/docs\"  # Update this to your actual directory path\n",
        "read_and_process_documents(docs_directory, COLLECTION)\n",
        "\n",
        "# Start interactive chat\n",
        "run_interactive_chat(COLLECTION)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
